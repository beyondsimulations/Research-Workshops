{
  "hash": "d5c5fb72c882667f95402f3322246d77",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Working with Large Language Models\"\nsubtitle: \"University of Hamburg - Spring 2025\"\nauthor: \"Dr. Tobias VlÄ‡ek\"\n\nhtml:\n    theme: [default, styles.scss]\n    toc: true\n    highlight-style: arrow\n    linkcolor: \"#a60000\"\n    code-copy: true\n    code-link: true\n    toc-location: right\n    code-overflow: wrap\n    code-block-bg: true\n    code-block-border-left: \"#AD1F00\"\n---\n\n\n\n\n\n\n## Welcome to today's workshop!\n\n- Explore Large Language Models (LLMs).\n- You've likely interacted with LLMs already:\n    - ChatGPT\n    - Google's Gemini\n    - Claude\n- Understand:\n    - How LLMs work on a high level.\n    - What you can do with them.\n\n## What is an LLM?\n\n- LLMs are a type of Artificial Intelligence (AI).\n- Specifically within Natural Language Processing (NLP).\n- Designed to:\n    - Understand human language.\n    - Generate human language.\n    - Interact with human language.\n    - In remarkably human-like ways.\n\n::: {.callout-tip}\n**Why should you care?** LLMs are rapidly transforming how we:\n\n- Write and edit content\n- Develop software\n- Conduct research\n- Analyze data\n- Automate tasks\n- Learn new concepts\n\nUnderstanding these tools is becoming essential across nearly every professional field.\n:::\n\n## A Great Overview by 3Blue1Brown\n\n- Greg Sanderson provides an excellent, concise explanation of LLMs.\n- Great starting point to understand LLMs.\n- Check out his [YouTube channel, 3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw) for more.\n\n\n\n\n\n\n{{< video https://www.youtube.com/embed/LPZh9BOjkQs >}}\n\n\n\n\n\n\n\n\n\n\n## Today's Key Concepts\n\n- Fundamental principles behind how LLMs work.\n- The process of training these powerful models.\n- Limitations and ethical considerations of LLMs.\n- Practical applications, including pair-programming.\n- How to use LLMs with your own data (Retrieval Augmented Generation).\n- Further resources and tools.\n\n# How they work\n\n- High-level overview of LLM workings.\n- No deep technical details.\n\n## Deep Learning and Text Data\n\n- LLMs are *deep learning* models.\n- Built using artificial neural networks with many layers (\"deep\").\n- Trained on *enormous* amounts of text data:\n    - Internet\n    - Books\n    - Articles\n    - Code\n    - And more!\n\n- Training allows them to learn statistical patterns of language.\n- Become incredibly good at predicting word sequences.\n- Foundation for tasks like:\n    - **Text Generation:** Creating new text similar to training data.\n    - **Translation:** Converting text between languages.\n    - **Summarization:** Condensing text into shorter summaries.\n    - **Question Answering:** Answering questions based on learned information.\n    - **Sentiment Analysis:** Determining emotional tone of text.\n\n## The Transformer\n\n- Modern LLMs powered by **transformer model** architecture.\n- Introduced in \"[Attention is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani et al., 2017).\n- Transformers revolutionized NLP.\n\n**How Transformers Work (Simplified):**\n\n1.  **Tokenization:** Input text broken into *tokens*.\n2.  **Mathematical Relationships:** Identifies relationships between tokens.\n    - Understands word/phrase connections in sentences and context.\n3.  **Attention Mechanism:** *Crucial* part.\n    - Focuses on most relevant input parts when generating output.\n    - Pays \"attention\" to important words for meaning.\n\n**Example: The Power of Attention**\n\n- Sentence: \"The cat sat on the mat because it was warm.\"\n- Attention mechanism understands:\n    - \"it\" refers to \"the mat,\" not \"the cat.\"\n    - By considering context of surrounding words and relationships.\n\n## Tokens: The Building Blocks\n\n- LLMs process text as **tokens**, not whole words.\n- Token can be:\n    - Whole word (e.g., \"cat\")\n    - Subword (e.g., \"un-\", \"break\", \"-able\" for \"unbreakable\")\n    - Individual characters\n\n**Why use tokens?**\n\n- **Handles Unknown Words:** Processes unseen words via subword units.\n- **Efficiency:** Learning relationships between tokens is more efficient.\n\n**Challenges with Tokenization:**\n\n- Tokenization can be tricky.\n- Especially for languages without spaces (Chinese, Japanese).\n- Current methods better for languages with similar scripts (English, Spanish, French).\n- Potentially disadvantages other languages.\n- Area of ongoing research.\n\n::: {.callout-note}\n*Think about this: How might this impact the fairness and inclusivity of LLMs?*\n:::\n\n## Context Window: LLM's \"Memory\"\n\n- **Context window:** LLM's short-term memory.\n- Number of tokens the model can process at once.\n- Larger context window:\n    - \"Remembers\" more of conversation/document.\n    - More coherent and contextually relevant responses.\n    - More computational resources needed.\n    - Potentially less coherence if *too* long.\n- Shorter context window:\n    - Faster processing.\n    - May lose track of earlier conversation parts.\n\n::: {.callout-tip}\nThere is a trade-off between the context window and the size of the model. A larger model can usually handle a larger context window, but it will also take more computational resources to run.\n:::\n\n# LLM Architectures\n\n- LLMs are versatile tools.\n- Different tasks need different architectures.\n- Main architectures designed for specific strengths.\n\n## Encoder-Decoder\n\n- **How it works:**\n    - **Encoder:** Processes input text -> condensed representation (\"thought vector\").\n    - **Decoder:** Takes representation -> generates output text.\n    - Two-step process: understand, then create.\n- **Analogy:** Translation.\n    - Understand English sentence (encoder).\n    - Construct German sentence (decoder).\n- **Key Feature:** Excellent for tasks with different input and output sequences.\n\n## Encoder-Only\n\n- **How it works:**\n    - Focuses on *understanding* input.\n    - Processes input -> rich representation of meaning.\n- **Analogy:** Detective analyzing crime scene.\n    - Gather clues (input) -> build complete picture.\n    - No long report needed (output).\n- **Key Feature:** Great for classification or information extraction.\n\n## Decoder-Only\n\n- **How it works:**\n    - Focuses on *generating* text.\n    - Takes prompt/conversation history -> predicts next word, then next, etc.\n    - Builds output sequence.\n- **Analogy:** Creative writer or conversation.\n    - Writer starts with sentence and continues story.\n    - Conversation builds upon previous turns.\n- **Key Feature:** Ideal for original text creation.\n\n### Mixture of Experts (MoE)\n\n- **How it works:**\n    - Uses multiple smaller \"expert\" models.\n    - Each expert specialized in different aspect.\n    - \"Gating network\" decides which expert(s) to use.\n- **Analogy:** Large company with departments.\n    - Manager (gating network) assigns project to relevant departments.\n- **Key Feature:** Efficient scaling to large models/datasets.\n    - Only subset of experts activated per input.\n    - Improves performance and reduces cost.\n    - Used in large chatbot models.\n\n# Training LLMs\n\n- Training an LLM: like teaching a child language, but huge scale.\n\n## The Training Process\n\n- LLMs learn via \"pre-training\" on massive text/code datasets.\n\n1. **Input Processing:**\n    - Model receives token sequence.\n    - Text broken into tokens (words, subwords, characters).\n\n2. **Prediction Task:**\n    - Given sequence, predict next token.\n    - Like filling in the blank: \"The cat sat on the ___\"\n\n3. **Learning Loop:**\n    - Make prediction.\n    - Compare to actual answer.\n    - Adjust internal parameters.\n    - Repeat billions of times.\n\n::: {.callout-note}\nThis process requires enormous computational resources - training a large model can cost millions of dollars in computing power!\n:::\n\n## Computational Power and Parallelism\n\n- Training needs *immense* computational power.\n- **Specialized hardware:** GPUs (Graphics Processing Units).\n- **Model Parallelism:** Distributes model parts across multiple GPUs.\n    - Speeds up training via parallel processing.\n    - Reason for Nvidia's stock increase.\n\n## Training Phases\n\n- Training process often divided into phases:\n\n- **Self-Supervised Learning:**\n    - Trained on massive dataset *without* explicit labels.\n    - Learns to predict next token.\n    - Learns basic language rules and patterns.\n    - Like learning grammar/vocab by reading books.\n- **Supervised Fine-tuning:**\n    - Trained on smaller, *labeled* dataset for specific tasks.\n    - Specialized for tasks like question answering, summarization.\n    - Like specialized course after learning basics.\n- **Reinforcement Learning from Human Feedback (RLHF):**\n    - Human feedback refines model output.\n    - Makes output more helpful, honest, and harmless.\n    - Aligns text generation with human preferences.\n    - Like teacher feedback to improve writing.\n\n# Limitations and Ethical Considerations\n\n- LLMs are powerful but not perfect.\n- Understand limitations and ethical implications.\n\n## Bias: A Reflection of the Data\n\n- LLMs can exhibit biases (gender, racial, cultural, etc.).\n- Learn biases from training data.\n- Biased training data -> biased output.\n\n**Example:**\n\n- Trained mostly on text by men -> reflect male perspectives/stereotypes.\n\n**Impact of Bias:**\n\n- Unfair/discriminatory outcomes.\n- Reinforcement of harmful stereotypes.\n- Erosion of trust in AI.\n\n**Mitigating Bias:**\n\n- **Data Curation:** Carefully select diverse training data. Include data from different demographics, languages, cultures.\n- **Model Fine-tuning:** Train on datasets to counteract bias.\n- **Auditing and Evaluation:** Rigorously test models for bias.\n\n## Adversarial Attacks: Tricking the Model\n\n- Adversarial attacks: small input changes to mislead LLM.\n- Exploit model vulnerabilities.\n\n**Example:**\n\n- Slightly change prompt wording -> biased/harmful/nonsensical response.\n\n**Why is this a problem?**\n\n- Manipulate LLMs for malicious purposes.\n- Spreading misinformation or generating harmful content.\n\n## A Broader View\n\n- LLMs raise broader ethical concerns:\n- **Job Displacement:** Automate human tasks -> potential job losses. \n    - *How to prepare for this shift?*\n- **Misinformation:** Generate realistic fake news easily. \n    - *How to combat misinformation spread?*\n- **Malicious Use:** Deepfakes, harmful content, propaganda. \n    - *What safeguards are needed?*\n- **Privacy**: Trained on personal data. \n    - *Data usage and privacy regulations needed?*\n- **Accountability**: Unclear who is responsible for LLM-generated content. \n    - *How to hold producers/users accountable?*\n- **Transparency:** Difficult to understand LLM decision-making. \n    - *How to make LLMs more transparent?*\n\n- Addressing ethical concerns requires: **researchers, policymakers, public**.\n\n# Putting LLMs to Work\n\n- Explore how to *use* LLMs practically.\n\n## Pair Programming with LLMs\n\n- Pair programming: two programmers work together.\n    - One writes code, other reviews/feedback.\n- LLMs as \"AI pair programmer\":\n    - **Code Completion:** Suggest code snippets, complete lines.\n    - **Error Detection:** Identify bugs, suggest fixes.\n    - **Code Generation:** Generate functions/blocks from instructions.\n    - **Code Explanation:** Explain how code works.\n    - **Best Practices:** Suggest improvements and best practices.\n\n**Tools for Pair Programming:**\n\n- **GitHub Copilot:** VS Code integration, AI-powered suggestions.\n- **Cursor:** VS Code fork, more powerful AI features.\n- **Zed:** Code editor for pair programming (LLMs and humans). Local and API models.\n\n## Tips for Effective Pair Programming\n\n- **Start Small:** Begin with code completion, then complex tasks.\n- **Be Specific:** Clear, concise instructions to LLM.\n- **Review Carefully:** *Always* review LLM-generated code. Mistakes can happen!\n- **Learn from the LLM:** Learn new techniques and coding styles.\n\n::: {.callout-tip}\n**Manage Context:** Be mindful of context window. For longer codebases, LLM may \"forget\" earlier parts. Break down large tasks.\n:::\n\n# Retrieval Augmented Generation (RAG)\n\n- Retrieval Augmented Generation (RAG): combines LLMs with external knowledge.\n- LLM with vast library and research assistant.\n\n**Why is RAG important?**\n\n- **Up-to-Date Information:** Access *current* information beyond training data.\n- **Reduced Hallucinations:** Responses grounded in facts, less making things up.\n- **Domain-Specific Knowledge:** Connect to your documents, databases, APIs. \n- **Expert in your area:** Tailored to specific domain without retraining.\n\n**How RAG Works:**\n\n1.  **Query:** Ask a question or provide prompt.\n2.  **Retrieval:** Search external sources for relevant info.\n3.  **Augmentation:** Add retrieved info to LLM's prompt (context).\n4.  **Generation:** LLM generates response based on:\n    - Pre-trained knowledge.\n    - Retrieved information.\n\n**Example:**\n\n- **Question:** \"What are latest advancements in quantum computing?\"\n- **Retrieval:** Search scientific publications, news, research databases.\n- **Augmentation:** Key findings added to prompt.\n- **Generation:** Comprehensive, up-to-date answer.\n\n# Running LLMs Locally with Ollama\n\n- Running LLMs locally: on your own computer.\n\n**Advantages:**\n\n- **Privacy:** Data stays on your machine.\n- **Cost Savings:** No API fees.\n- **Customization:** Experiment, fine-tune for needs.\n- **Offline Access:** Use without internet.\n- Requires understanding hardware and technical setup.\n- **Ollama**: user-friendly tool for local LLMs.\n\n## Ollama\n\n- [Ollama](https://ollama.com/): free, open-source.\n- Simplifies downloading, installing, running LLMs (macOS, Linux, Windows).\n- Command-line interface (CLI), wide model support.\n\n**Key Features of Ollama:**\n\n- **Easy Installation:** Simple download and install.\n- **Model Management:** Download/manage models from library (Hugging Face).\n- **Command-Line Interface:** Interact via commands.\n- **API Server:** Local API server, integrate in applications.\n\n## Hardware Requirements\n\n- Local LLMs need sufficient resources:\n\n| Component | Requirement | Notes |\n|-----------|-------------|--------|\n| RAM | Minimum 8GB | - 16GB recommended<br>- 32GB+ for larger models |\n| CPU | Modern multi-core | - Can run smaller models (< 7B params)<br>- Faster = better performance |\n| GPU | VRAM dependent | - More VRAM = larger models<br>- NVIDIA GPUs preferred |\n| Storage | 10GB+ free | - Models can be several GB each |\n\n::: {.callout-warning}\n**Performance Note:** CPU-only can be much slower compared to GPU.\n:::\n\n## Billions of Parameters (B)\n\n- LLMs described by *parameters* (internal learned values).\n- More parameters -> more capable (generally), larger model, more resources.\n- Model size not everything, quality depends on data/training/architecture.\n- Newer models can be more efficient.\n\n## Size Overview\n\n- **Small Models (< 7B):** CPU with 16GB+ RAM. Experimentation, less demanding tasks.\n- **Medium Models (7B - 13B):** Benefit from GPU, sometimes CPU with 32GB+ RAM.\n- **Large Models (30B+):** Powerful GPU with VRAM needed.\n- **Very Large Models (70B+):** High-end/multiple GPUs.\n\n::: {.callout-tip}\nModel names like `mistral:7b`, `codellama:34b` - \"b\" = billions of parameters.\n:::\n\n## Quantization: Making Models Smaller\n\n- **Quantization:** Reduce size/compute of LLMs \"without significantly impacting performance\".\n- Represent parameters with lower precision (e.g., 8-bit, 4-bit integers).\n\n**Benefits of Quantization:**\n\n- **Smaller Model Size:** Less RAM/disk needed.\n- **Faster Inference:** Faster low-precision calculations.\n- **Lower Power Consumption:** Less hardware demand.\n- **Trade-offs:** Small accuracy decrease possible. More aggressive quantization = more performance impact.\n- **Ollama and Quantization**: Supports quantized models from Hugging Face.\n    - `q4_0`, `q8_0` suffixes indicate quantization level.\n    - `q4_0`: 4-bit (balance size/quality).\n    - `q8_0`: 8-bit (closer to original, larger size).\n\n::: {.callout-tip}\nQuantization levels offer different balance of speed, size, quality. Experiment to find best option for your hardware/needs. Default model often sufficient, quantization not always needed.\n:::\n\n## Getting Started with Ollama\n\n1.  **Download and Install Ollama:** [ollama.com](https://ollama.com/).\n2.  **Pull a Model:** `ollama pull mistral:7b` (terminal/command prompt).\n3.  **Run the Model:** `ollama run mistral:7b` (terminal). Chat interface opens.\n4.  **Ask a Question:** Type prompt and press Enter.\n5.  **Experiment:** Try different models from the Ollama website model library. Use Hugging Face for more models/quantization.\n\n## Talk to the LLM in code\n\n- To talk to the LLM in code, you can use the following code\n- Make sure that you have a local Ollama with the model you want to use running!\n\n::: {#c4967092 .cell execution_count=1}\n``` {.python .cell-code}\nimport requests\nimport json\n\ndef ask_llm(prompt, model=\"mistral:7b\"):\n    \"\"\"\n    Send a prompt to a locally running Ollama model and get the response.\n    \n    Args:\n        prompt (str): The question or prompt to send to the model\n        model (str): The name of the model to use (default: \"mistral:7b\")\n    \n    Returns:\n        str: The model's response\n    \"\"\"\n\n    url = \"http://localhost:11434/api/generate\"\n    \n    data = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"stream\": False\n    }\n    \n    try:\n        response = requests.post(url, json=data)\n        response.raise_for_status()  # Raise an exception for bad status\n        result = response.json()\n        return result[\"response\"]\n    except requests.exceptions.RequestException as e:\n        return f\"Error: {str(e)}\\nMake sure Ollama is running locally!\"\n\ndef main():\n    print(\"Local LLM Chat (type 'quit' to exit)\")\n    print(\"-\" * 50)\n    \n    while True:\n        user_input = input(\"\\nYour question: \")\n        \n        if user_input.lower() in ['quit', 'exit']:\n            print(\"\\nGoodbye!\")\n            break\n            \n        response = ask_llm(user_input)\n        print(\"\\nResponse:\", response)\n\nif __name__ == \"__main__\":\n    main()\n```\n:::\n\n\n# Wrap-up\n\n- LLMs are powerful tools for many tasks.\n- Understand their capabilities and limitations.\n- Use them effectively in your work.\n\n",
    "supporting": [
      "large-language-models_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}