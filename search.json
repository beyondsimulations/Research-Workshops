[
  {
    "objectID": "general/literature.html",
    "href": "general/literature.html",
    "title": "Literature and Resources",
    "section": "",
    "text": "This section provides a curated list of books and resources to enhance your understanding of Large Language Models. Each recommendation includes a brief description to help you choose the most suitable resources for you.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "general/literature.html#hosted-llms",
    "href": "general/literature.html#hosted-llms",
    "title": "Literature and Resources",
    "section": "Hosted LLMs",
    "text": "Hosted LLMs\nThese are powerful LLMs hosted by companies, which you can access through APIs (Application Programming Interfaces). You typically pay for usage.\n\nOpenAI (ChatGPT): The creators of ChatGPT and GPT-4, offering a range of models.\nMistral: A European-based company offering competitive models.\nGoogle (Gemini): Google’s LLM, offering strong performance and integration with Google services.\nAnthropic (Claude): Known for its its ability to handle code effectively.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "general/literature.html#local-llms",
    "href": "general/literature.html#local-llms",
    "title": "Literature and Resources",
    "section": "Local LLMs",
    "text": "Local LLMs\nThese tools can be used to run open-source LLMs that you can download and run on your own machine. This gives you more privacy and control, but requires more technical expertise and computational resources.\n\nOllama: Free and open-source tool to run large language models locally, supports a wide range of models. Note, that the models are not as powerful as the hosted ones and that your computer needs to have a good GPU to run larger models. Smaller models with less than 8B parameters can also often be run on a CPU with enough available RAM. Great for privacy and if you don’t want to pay for the hosted models.\nHugging Face: Hosts a wide range of large language models, including models fine-tuned for specific tasks by the community. Models can also be downloaded to Ollama and run locally, if your computer is powerful enough.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "general/literature.html#working-with-data",
    "href": "general/literature.html#working-with-data",
    "title": "Literature and Resources",
    "section": "Working with data",
    "text": "Working with data\nIn addition to the hosted and local LLMs, there are also tools that allow you to work with LLMs in a browser to build RAG apps or custom chatbots.\n\nNotebookLM**: Google’s Gemini that can be fed with files, images and YouTube videos to generate text based on the content. Only works within a workspace of Google, you can’t make it available to the public (yet).\nLM Studio: An application for discovering, downloading, and running LLMs locally. It supports various model architectures and offers both a Chat UI and an OpenAI-compatible local server. Features include offline document chat capabilities and easy model downloads from Hugging Face.\nOpen Web UI**: Open Web UI is a tool to run large language models locally (in conjuction with, for example, Ollama). It is a browser-based interface that allows you to interact with the models and build RAG apps.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "general/literature.html#further-resources",
    "href": "general/literature.html#further-resources",
    "title": "Literature and Resources",
    "section": "Further resources",
    "text": "Further resources\n\nQuarto\n\nA static website generator that is very powerful and flexible. Used to create the slides and the website for the course.\n\nGithub\n\nThe largest provider for git repositories owned by Microsoft. A lot of open source projects are hosted here and you can read the code.\n\nDaily Dose of Data Science\n\nA website and a newsletter with lots of easy-to-digest resources to improve your skills in Data Science and Large Language Models.",
    "crumbs": [
      "Literature"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the workshops!",
    "section": "",
    "text": "Description\nWelcome to this collection of resources! This website hosts a collection of information and resources that will help you prepare for your research. If you are interested in large language models, these materials are designed for you. The material is self-contained and provides you with the fundamental knowledge and practical skills needed for your research project.\nThe resources are mostly available on this website with links to some other resources. I will try to update the website in the future, to extend the content and reflect the latest developments in the field.\n\n\nQuestions\nIf you have any questions, please contact me under tobias.vlcek@uni-hamburg.de.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "large-language-models.html",
    "href": "large-language-models.html",
    "title": "Working with Large Language Models",
    "section": "",
    "text": "Explore Large Language Models (LLMs).\nYou’ve likely interacted with LLMs already:\n\nChatGPT\nGoogle’s Gemini\nClaude\n\nUnderstand:\n\nHow LLMs work on a high level.\nWhat you can do with them.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#welcome-to-todays-workshop",
    "href": "large-language-models.html#welcome-to-todays-workshop",
    "title": "Working with Large Language Models",
    "section": "",
    "text": "Explore Large Language Models (LLMs).\nYou’ve likely interacted with LLMs already:\n\nChatGPT\nGoogle’s Gemini\nClaude\n\nUnderstand:\n\nHow LLMs work on a high level.\nWhat you can do with them.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#what-is-an-llm",
    "href": "large-language-models.html#what-is-an-llm",
    "title": "Working with Large Language Models",
    "section": "What is an LLM?",
    "text": "What is an LLM?\n\nLLMs are a type of Artificial Intelligence (AI).\nSpecifically within Natural Language Processing (NLP).\nDesigned to:\n\nUnderstand human language.\nGenerate human language.\nInteract with human language.\nIn remarkably human-like ways.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhy should you care? LLMs are rapidly transforming how we:\n\nWrite and edit content\nDevelop software\nConduct research\nAnalyze data\nAutomate tasks\nLearn new concepts\n\nUnderstanding these tools is becoming essential across nearly every professional field.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#a-great-overview-by-3blue1brown",
    "href": "large-language-models.html#a-great-overview-by-3blue1brown",
    "title": "Working with Large Language Models",
    "section": "A Great Overview by 3Blue1Brown",
    "text": "A Great Overview by 3Blue1Brown\n\nGreg Sanderson provides an excellent, concise explanation of LLMs.\nGreat starting point to understand LLMs.\nCheck out his YouTube channel, 3Blue1Brown for more.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#todays-key-concepts",
    "href": "large-language-models.html#todays-key-concepts",
    "title": "Working with Large Language Models",
    "section": "Today’s Key Concepts",
    "text": "Today’s Key Concepts\n\nFundamental principles behind how LLMs work.\nThe process of training these powerful models.\nLimitations and ethical considerations of LLMs.\nPractical applications, including pair-programming.\nHow to use LLMs with your own data (Retrieval Augmented Generation).\nFurther resources and tools.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#deep-learning-and-text-data",
    "href": "large-language-models.html#deep-learning-and-text-data",
    "title": "Working with Large Language Models",
    "section": "Deep Learning and Text Data",
    "text": "Deep Learning and Text Data\n\nLLMs are deep learning models.\nBuilt using artificial neural networks with many layers (“deep”).\nTrained on enormous amounts of text data:\n\nInternet\nBooks\nArticles\nCode\nAnd more!\n\nTraining allows them to learn statistical patterns of language.\nBecome incredibly good at predicting word sequences.\nFoundation for tasks like:\n\nText Generation: Creating new text similar to training data.\nTranslation: Converting text between languages.\nSummarization: Condensing text into shorter summaries.\nQuestion Answering: Answering questions based on learned information.\nSentiment Analysis: Determining emotional tone of text.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#the-transformer",
    "href": "large-language-models.html#the-transformer",
    "title": "Working with Large Language Models",
    "section": "The Transformer",
    "text": "The Transformer\n\nModern LLMs powered by transformer model architecture.\nIntroduced in “Attention is All You Need” (Vaswani et al., 2017).\nTransformers revolutionized NLP.\n\nHow Transformers Work (Simplified):\n\nTokenization: Input text broken into tokens.\nMathematical Relationships: Identifies relationships between tokens.\n\nUnderstands word/phrase connections in sentences and context.\n\nAttention Mechanism: Crucial part.\n\nFocuses on most relevant input parts when generating output.\nPays “attention” to important words for meaning.\n\n\nExample: The Power of Attention\n\nSentence: “The cat sat on the mat because it was warm.”\nAttention mechanism understands:\n\n“it” refers to “the mat,” not “the cat.”\nBy considering context of surrounding words and relationships.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#tokens-the-building-blocks",
    "href": "large-language-models.html#tokens-the-building-blocks",
    "title": "Working with Large Language Models",
    "section": "Tokens: The Building Blocks",
    "text": "Tokens: The Building Blocks\n\nLLMs process text as tokens, not whole words.\nToken can be:\n\nWhole word (e.g., “cat”)\nSubword (e.g., “un-”, “break”, “-able” for “unbreakable”)\nIndividual characters\n\n\nWhy use tokens?\n\nHandles Unknown Words: Processes unseen words via subword units.\nEfficiency: Learning relationships between tokens is more efficient.\n\nChallenges with Tokenization:\n\nTokenization can be tricky.\nEspecially for languages without spaces (Chinese, Japanese).\nCurrent methods better for languages with similar scripts (English, Spanish, French).\nPotentially disadvantages other languages.\nArea of ongoing research.\n\n\n\n\n\n\n\nNote\n\n\n\nThink about this: How might this impact the fairness and inclusivity of LLMs?",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#context-window-llms-memory",
    "href": "large-language-models.html#context-window-llms-memory",
    "title": "Working with Large Language Models",
    "section": "Context Window: LLM’s “Memory”",
    "text": "Context Window: LLM’s “Memory”\n\nContext window: LLM’s short-term memory.\nNumber of tokens the model can process at once.\nLarger context window:\n\n“Remembers” more of conversation/document.\nMore coherent and contextually relevant responses.\nMore computational resources needed.\nPotentially less coherence if too long.\n\nShorter context window:\n\nFaster processing.\nMay lose track of earlier conversation parts.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere is a trade-off between the context window and the size of the model. A larger model can usually handle a larger context window, but it will also take more computational resources to run.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#encoder-decoder",
    "href": "large-language-models.html#encoder-decoder",
    "title": "Working with Large Language Models",
    "section": "Encoder-Decoder",
    "text": "Encoder-Decoder\n\nHow it works:\n\nEncoder: Processes input text -&gt; condensed representation (“thought vector”).\nDecoder: Takes representation -&gt; generates output text.\nTwo-step process: understand, then create.\n\nAnalogy: Translation.\n\nUnderstand English sentence (encoder).\nConstruct German sentence (decoder).\n\nKey Feature: Excellent for tasks with different input and output sequences.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#encoder-only",
    "href": "large-language-models.html#encoder-only",
    "title": "Working with Large Language Models",
    "section": "Encoder-Only",
    "text": "Encoder-Only\n\nHow it works:\n\nFocuses on understanding input.\nProcesses input -&gt; rich representation of meaning.\n\nAnalogy: Detective analyzing crime scene.\n\nGather clues (input) -&gt; build complete picture.\nNo long report needed (output).\n\nKey Feature: Great for classification or information extraction.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#decoder-only",
    "href": "large-language-models.html#decoder-only",
    "title": "Working with Large Language Models",
    "section": "Decoder-Only",
    "text": "Decoder-Only\n\nHow it works:\n\nFocuses on generating text.\nTakes prompt/conversation history -&gt; predicts next word, then next, etc.\nBuilds output sequence.\n\nAnalogy: Creative writer or conversation.\n\nWriter starts with sentence and continues story.\nConversation builds upon previous turns.\n\nKey Feature: Ideal for original text creation.\n\n\nMixture of Experts (MoE)\n\nHow it works:\n\nUses multiple smaller “expert” models.\nEach expert specialized in different aspect.\n“Gating network” decides which expert(s) to use.\n\nAnalogy: Large company with departments.\n\nManager (gating network) assigns project to relevant departments.\n\nKey Feature: Efficient scaling to large models/datasets.\n\nOnly subset of experts activated per input.\nImproves performance and reduces cost.\nUsed in large chatbot models.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#the-training-process",
    "href": "large-language-models.html#the-training-process",
    "title": "Working with Large Language Models",
    "section": "The Training Process",
    "text": "The Training Process\n\nLLMs learn via “pre-training” on massive text/code datasets.\n\n\nInput Processing:\n\nModel receives token sequence.\nText broken into tokens (words, subwords, characters).\n\nPrediction Task:\n\nGiven sequence, predict next token.\nLike filling in the blank: “The cat sat on the ___”\n\nLearning Loop:\n\nMake prediction.\nCompare to actual answer.\nAdjust internal parameters.\nRepeat billions of times.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis process requires enormous computational resources - training a large model can cost millions of dollars in computing power!",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#computational-power-and-parallelism",
    "href": "large-language-models.html#computational-power-and-parallelism",
    "title": "Working with Large Language Models",
    "section": "Computational Power and Parallelism",
    "text": "Computational Power and Parallelism\n\nTraining needs immense computational power.\nSpecialized hardware: GPUs (Graphics Processing Units).\nModel Parallelism: Distributes model parts across multiple GPUs.\n\nSpeeds up training via parallel processing.\nReason for Nvidia’s stock increase.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#training-phases",
    "href": "large-language-models.html#training-phases",
    "title": "Working with Large Language Models",
    "section": "Training Phases",
    "text": "Training Phases\n\nTraining process often divided into phases:\nSelf-Supervised Learning:\n\nTrained on massive dataset without explicit labels.\nLearns to predict next token.\nLearns basic language rules and patterns.\nLike learning grammar/vocab by reading books.\n\nSupervised Fine-tuning:\n\nTrained on smaller, labeled dataset for specific tasks.\nSpecialized for tasks like question answering, summarization.\nLike specialized course after learning basics.\n\nReinforcement Learning from Human Feedback (RLHF):\n\nHuman feedback refines model output.\nMakes output more helpful, honest, and harmless.\nAligns text generation with human preferences.\nLike teacher feedback to improve writing.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#bias-a-reflection-of-the-data",
    "href": "large-language-models.html#bias-a-reflection-of-the-data",
    "title": "Working with Large Language Models",
    "section": "Bias: A Reflection of the Data",
    "text": "Bias: A Reflection of the Data\n\nLLMs can exhibit biases (gender, racial, cultural, etc.).\nLearn biases from training data.\nBiased training data -&gt; biased output.\n\nExample:\n\nTrained mostly on text by men -&gt; reflect male perspectives/stereotypes.\n\nImpact of Bias:\n\nUnfair/discriminatory outcomes.\nReinforcement of harmful stereotypes.\nErosion of trust in AI.\n\nMitigating Bias:\n\nData Curation: Carefully select diverse training data. Include data from different demographics, languages, cultures.\nModel Fine-tuning: Train on datasets to counteract bias.\nAuditing and Evaluation: Rigorously test models for bias.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#adversarial-attacks-tricking-the-model",
    "href": "large-language-models.html#adversarial-attacks-tricking-the-model",
    "title": "Working with Large Language Models",
    "section": "Adversarial Attacks: Tricking the Model",
    "text": "Adversarial Attacks: Tricking the Model\n\nAdversarial attacks: small input changes to mislead LLM.\nExploit model vulnerabilities.\n\nExample:\n\nSlightly change prompt wording -&gt; biased/harmful/nonsensical response.\n\nWhy is this a problem?\n\nManipulate LLMs for malicious purposes.\nSpreading misinformation or generating harmful content.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#a-broader-view",
    "href": "large-language-models.html#a-broader-view",
    "title": "Working with Large Language Models",
    "section": "A Broader View",
    "text": "A Broader View\n\nLLMs raise broader ethical concerns:\nJob Displacement: Automate human tasks -&gt; potential job losses.\n\nHow to prepare for this shift?\n\nMisinformation: Generate realistic fake news easily.\n\nHow to combat misinformation spread?\n\nMalicious Use: Deepfakes, harmful content, propaganda.\n\nWhat safeguards are needed?\n\nPrivacy: Trained on personal data.\n\nData usage and privacy regulations needed?\n\nAccountability: Unclear who is responsible for LLM-generated content.\n\nHow to hold producers/users accountable?\n\nTransparency: Difficult to understand LLM decision-making.\n\nHow to make LLMs more transparent?\n\nAddressing ethical concerns requires: researchers, policymakers, public.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#pair-programming-with-llms",
    "href": "large-language-models.html#pair-programming-with-llms",
    "title": "Working with Large Language Models",
    "section": "Pair Programming with LLMs",
    "text": "Pair Programming with LLMs\n\nPair programming: two programmers work together.\n\nOne writes code, other reviews/feedback.\n\nLLMs as “AI pair programmer”:\n\nCode Completion: Suggest code snippets, complete lines.\nError Detection: Identify bugs, suggest fixes.\nCode Generation: Generate functions/blocks from instructions.\nCode Explanation: Explain how code works.\nBest Practices: Suggest improvements and best practices.\n\n\nTools for Pair Programming:\n\nGitHub Copilot: VS Code integration, AI-powered suggestions.\nCursor: VS Code fork, more powerful AI features.\nZed: Code editor for pair programming (LLMs and humans). Local and API models.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#tips-for-effective-pair-programming",
    "href": "large-language-models.html#tips-for-effective-pair-programming",
    "title": "Working with Large Language Models",
    "section": "Tips for Effective Pair Programming",
    "text": "Tips for Effective Pair Programming\n\nStart Small: Begin with code completion, then complex tasks.\nBe Specific: Clear, concise instructions to LLM.\nReview Carefully: Always review LLM-generated code. Mistakes can happen!\nLearn from the LLM: Learn new techniques and coding styles.\n\n\n\n\n\n\n\nTip\n\n\n\nManage Context: Be mindful of context window. For longer codebases, LLM may “forget” earlier parts. Break down large tasks.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#ollama",
    "href": "large-language-models.html#ollama",
    "title": "Working with Large Language Models",
    "section": "Ollama",
    "text": "Ollama\n\nOllama: free, open-source.\nSimplifies downloading, installing, running LLMs (macOS, Linux, Windows).\nCommand-line interface (CLI), wide model support.\n\nKey Features of Ollama:\n\nEasy Installation: Simple download and install.\nModel Management: Download/manage models from library (Hugging Face).\nCommand-Line Interface: Interact via commands.\nAPI Server: Local API server, integrate in applications.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#hardware-requirements",
    "href": "large-language-models.html#hardware-requirements",
    "title": "Working with Large Language Models",
    "section": "Hardware Requirements",
    "text": "Hardware Requirements\n\nLocal LLMs need sufficient resources:\n\n\n\n\n\n\n\n\n\nComponent\nRequirement\nNotes\n\n\n\n\nRAM\nMinimum 8GB\n- 16GB recommended- 32GB+ for larger models\n\n\nCPU\nModern multi-core\n- Can run smaller models (&lt; 7B params)- Faster = better performance\n\n\nGPU\nVRAM dependent\n- More VRAM = larger models- NVIDIA GPUs preferred\n\n\nStorage\n10GB+ free\n- Models can be several GB each\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPerformance Note: CPU-only can be much slower compared to GPU.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#billions-of-parameters-b",
    "href": "large-language-models.html#billions-of-parameters-b",
    "title": "Working with Large Language Models",
    "section": "Billions of Parameters (B)",
    "text": "Billions of Parameters (B)\n\nLLMs described by parameters (internal learned values).\nMore parameters -&gt; more capable (generally), larger model, more resources.\nModel size not everything, quality depends on data/training/architecture.\nNewer models can be more efficient.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#size-overview",
    "href": "large-language-models.html#size-overview",
    "title": "Working with Large Language Models",
    "section": "Size Overview",
    "text": "Size Overview\n\nSmall Models (&lt; 7B): CPU with 16GB+ RAM. Experimentation, less demanding tasks.\nMedium Models (7B - 13B): Benefit from GPU, sometimes CPU with 32GB+ RAM.\nLarge Models (30B+): Powerful GPU with VRAM needed.\nVery Large Models (70B+): High-end/multiple GPUs.\n\n\n\n\n\n\n\nTip\n\n\n\nModel names like mistral:7b, codellama:34b - “b” = billions of parameters.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#quantization-making-models-smaller",
    "href": "large-language-models.html#quantization-making-models-smaller",
    "title": "Working with Large Language Models",
    "section": "Quantization: Making Models Smaller",
    "text": "Quantization: Making Models Smaller\n\nQuantization: Reduce size/compute of LLMs “without significantly impacting performance”.\nRepresent parameters with lower precision (e.g., 8-bit, 4-bit integers).\n\nBenefits of Quantization:\n\nSmaller Model Size: Less RAM/disk needed.\nFaster Inference: Faster low-precision calculations.\nLower Power Consumption: Less hardware demand.\nTrade-offs: Small accuracy decrease possible. More aggressive quantization = more performance impact.\nOllama and Quantization: Supports quantized models from Hugging Face.\n\nq4_0, q8_0 suffixes indicate quantization level.\nq4_0: 4-bit (balance size/quality).\nq8_0: 8-bit (closer to original, larger size).\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuantization levels offer different balance of speed, size, quality. Experiment to find best option for your hardware/needs. Default model often sufficient, quantization not always needed.",
    "crumbs": [
      "Large Language Models"
    ]
  },
  {
    "objectID": "large-language-models.html#getting-started-with-ollama",
    "href": "large-language-models.html#getting-started-with-ollama",
    "title": "Working with Large Language Models",
    "section": "Getting Started with Ollama",
    "text": "Getting Started with Ollama\n\nDownload and Install Ollama: ollama.com.\nPull a Model: ollama pull mistral:7b (terminal/command prompt).\nRun the Model: ollama run mistral:7b (terminal). Chat interface opens.\nAsk a Question: Type prompt and press Enter.\nExperiment: Try different models from the Ollama website model library. Use Hugging Face for more models/quantization.",
    "crumbs": [
      "Large Language Models"
    ]
  }
]